{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import pandas as pd\n",
    "import time\n",
    "import tiktoken\n",
    "import requests  # Para fazer requisições à API da DeepSeek\n",
    "from flask import Flask, request, jsonify\n",
    "import os\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Carrega os embeddings pré-treinados (se existirem)\n",
    "dataframe = pd.DataFrame()\n",
    "if 'training_embeddings.csv' in os.listdir():\n",
    "    dataframe = pd.read_csv('training_embeddings.csv', index_col=0)\n",
    "    def string_to_array(x):\n",
    "        return np.array(ast.literal_eval(x))    \n",
    "    dataframe['embedding'] = dataframe['embedding'].apply(string_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar a chave da API da DeepSeek\n",
    "def get_deepseek_api_key():\n",
    "    with open('deepseek_key.txt', 'r') as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "# Carrega a chave da API da DeepSeek\n",
    "deepseek_api_key = get_deepseek_api_key()\n",
    "print('DeepSeek API key loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para fazer uma requisição à API da DeepSeek\n",
    "def query_deepseek_api(prompt):\n",
    "    url = \"https://api.deepseek.com/v1/chat/completions\"  # Endpoint da DeepSeek\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {deepseek_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"deepseek-chat\",  # Modelo da DeepSeek\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_tokens\": 150,\n",
    "        \"top_p\": 0.7,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"stop\": ['###']\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinar o modelo\n",
    "def train():\n",
    "    filename = 'training.txt'\n",
    "    print('Training with data from', filename)\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "        data = data.lower().split('\\n')\n",
    "    \n",
    "    dataframe = pd.DataFrame(data, columns=['text'])\n",
    "    dataframe.to_csv('training.csv')\n",
    "    print('Training data saved to training.csv')\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    dataframe = pd.read_csv('training.csv', index_col=0)\n",
    "    dataframe.columns = ['text']\n",
    "\n",
    "    dataframe['n_tokens'] = dataframe['text'].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "\n",
    "    print('Training data loaded')\n",
    "\n",
    "    max_tokens = 512\n",
    "\n",
    "    def split_into_many(text, max_tokens = max_tokens):\n",
    "        sentences = text.split(' ')\n",
    "        n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "\n",
    "        chunks = []\n",
    "        tokens_so_far = 0\n",
    "        chunk = []\n",
    "\n",
    "        for sentence, token in zip(sentences, n_tokens):\n",
    "            if tokens_so_far + token > max_tokens:\n",
    "                chunks.append(\". \".join(chunk)+ \".\")\n",
    "                chunk = []\n",
    "                tokens_so_far = 0\n",
    "        \n",
    "            if token > max_tokens:\n",
    "                continue\n",
    "\n",
    "            chunk.append(sentence)\n",
    "            tokens_so_far += token + 1\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    shortened = []\n",
    "\n",
    "    for row in dataframe.iterrows():\n",
    "        if row[1]['text'] is None:\n",
    "            continue\n",
    "            \n",
    "        if row[1]['n_tokens'] > max_tokens:\n",
    "            shortened += split_into_many(row[1]['text'])\n",
    "        else:\n",
    "            shortened.append(row[1]['text'])\n",
    "\n",
    "    dataframe = pd.DataFrame(shortened, columns=['text'])\n",
    "    dataframe['n_tokens'] = dataframe['text'].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "\n",
    "    total_tokens = dataframe['n_tokens'].sum()\n",
    "    print('Total tokens:', total_tokens)\n",
    "    print('total embedding cost:', total_tokens/1000 * 0.0001)\n",
    "\n",
    "    i = 0\n",
    "    embeddings = []\n",
    "    for text in dataframe['text']:\n",
    "        try:\n",
    "            print('Creating embedding', i)\n",
    "            embedding = openai.Embedding.create(input=text, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
    "            embeddings.append(embedding)\n",
    "            time.sleep(1)\n",
    "        except openai.error.RateLimitError:\n",
    "            print('Rate limit reached, sleeping for 1 minute')\n",
    "            time.sleep(20)\n",
    "            embedding = openai.Embedding.create(input=text, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
    "            embeddings.append(embedding)\n",
    "        i += 1\n",
    "    \n",
    "    print('Embeddings created')\n",
    "    dataframe['embedding'] = embeddings\n",
    "    dataframe.to_csv('training_embeddings.csv')\n",
    "    print('Embeddings saved to training_embeddings.csv')\n",
    "    print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar contexto\n",
    "def create_context(question, df, max_len=1800, size=\"ada\"):\n",
    "    print('Creating context for question:', question)\n",
    "    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
    "    df['distances'] = df['embedding'].apply(lambda x: cosine(q_embeddings, x))\n",
    "    returns = []\n",
    "    cur_len = 0\n",
    "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
    "        cur_len += row['n_tokens'] + 4\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "        returns.append(row['text'])\n",
    "    return \"\\n###\\n\".join(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para responder perguntas usando a API da DeepSeek\n",
    "def answer(question):\n",
    "    context = create_context(question, dataframe)\n",
    "    prompt = f\"Você é um assistente para o site da faculdade, seja profissional e responda as perguntas baseado no contexto abaixo, recuse educadamente caso não possa responder.\\n{context}\\n---\\nPergunta: {question}\\nResposta:\"\n",
    "    \n",
    "    try:\n",
    "        response = query_deepseek_api(prompt)\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 'Minha cabeça está doendo, não consigo responder a essa pergunta agora.'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint para responder perguntas\n",
    "@app.route('/answer', methods=['POST'])\n",
    "def get_answer():\n",
    "    question = request.json['question']\n",
    "    return jsonify({'answer': answer(question)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint para treinar o modelo\n",
    "@app.route('/train', methods=['POST'])\n",
    "def train_model():\n",
    "    train()\n",
    "    return jsonify({'status': 'Training complete'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia o servidor Flask\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=2024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
